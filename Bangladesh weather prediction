{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7276003,"sourceType":"datasetVersion","datasetId":170193}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    df = pd.read_csv('/kaggle/input/bangladesh-weather-dataset/Temp_and_rain.csv')\nexcept FileNotFoundError:\n    print(\"Dataset file not found. Please update the file path.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a dummy dataframe for demonstration if the file is not found\ndates = pd.date_range(start='1901-01-01', end='2023-12-31', freq='M')\ndata = {\n        'Date': dates,\n        'Temperature': np.random.uniform(15, 35, size=len(dates)) + np.sin(np.arange(len(dates))/12) * 5,\n        'Humidity': np.random.uniform(50, 95, size=len(dates))\n    }\ndf = pd.DataFrame(data)\nprint(\"Using a dummy dataset for demonstration purposes.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert 'Date' column to datetime objects\ndf['Date'] = pd.to_datetime(df['Date'])\n# Set the date as the index of the dataframe\ndf.set_index('Date', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We will focus on forecasting the 'Temperature'.\n# Let's create a new dataframe with only this column.\ndata = df[['Temperature']].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Handle any missing values using forward-fill\ndata.ffill(inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 3: Scale the Data ---\n# LSTMs are sensitive to the scale of the input data, so we scale it to the range [0, 1].\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 4: Create Training and Testing Datasets ---\n# We will use the first 80% of the data for training and the last 20% for testing.\ntraining_data_len = int(len(scaled_data) * 0.8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = scaled_data[0:training_data_len, :]\ntest_data = scaled_data[training_data_len - 60:, :] # Include 60 previous points for the first test prediction","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 5: Create Sequences for Training and Testing ---\n# This function will create datasets where X is a sequence of past values and y is the next value.\ndef create_dataset(dataset, time_step=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset) - time_step - 1):\n        a = dataset[i:(i + time_step), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + time_step, 0])\n    return np.array(dataX), np.array(dataY)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We will use the last 60 days of data to predict the 61st day.\ntime_step = 60\nX_train, y_train = create_dataset(train_data, time_step)\nX_test, y_test = create_dataset(test_data, time_step)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- Step 6: Reshape Data for LSTM Model ---\n# The LSTM model expects input data in the shape [samples, time_steps, features].\n# Our data currently has 1 feature (Temperature).\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 7: Build and Compile the LSTM Model ---\nmodel = Sequential()\nmodel.add(LSTM(100, return_sequences=True, input_shape=(time_step, 1)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(50, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(25, activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 8: Train the Model ---\nprint(\"\\n--- Starting Model Training ---\")\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64, verbose=1)\nprint(\"--- Model Training Complete ---\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 9: Make Predictions ---\ntrain_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 10: Inverse Transform the Predictions ---\n# We need to revert the scaling to see the actual temperature values.\ntrain_predict = scaler.inverse_transform(train_predict)\ntest_predict = scaler.inverse_transform(test_predict)\ny_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 11: Evaluate the Model ---\nrmse_train = np.sqrt(mean_squared_error(y_train_inv, train_predict))\nrmse_test = np.sqrt(mean_squared_error(y_test_inv, test_predict))\nprint(f\"Train RMSE: {rmse_train:.4f}\")\nprint(f\"Test RMSE: {rmse_test:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 2 (Corrected): Load and Prepare the REAL Dataset ---\ndf = pd.read_csv('/kaggle/input/bangladesh-weather-dataset/Temp_and_rain.csv')\n\n# Create a proper 'Date' column and set it as the index\ndf['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str))\ndf.set_index('Date', inplace=True)\ndf.sort_index(inplace=True) # Ensure data is in chronological order\n\n# We will focus on forecasting the 'tem' (temperature).\n# Rename the column to 'Temperature' to match the rest of the code.\ndata = df[['tem']].copy()\ndata.rename(columns={'tem': 'Temperature'}, inplace=True)\n\n# Handle any missing values using forward-fill\ndata.ffill(inplace=True)\n\n\n# --- Step 12 (Corrected): Visualize the Results ---\n# The original code had an error in slicing the test_dates.\n# This corrected version ensures the number of dates matches the number of predictions.\n\nplt.style.use('seaborn-v0_8-whitegrid')\nfig, ax = plt.subplots(figsize=(16, 8))\n\n# Correctly select the dates that correspond to the test predictions\nnum_predictions = len(test_predict)\ntest_dates = data.index[-num_predictions:]\n\nax.plot(data.index, data['Temperature'], label='Full Historical Data', color='gray', alpha=0.6)\nax.plot(test_dates, y_test_inv, label='Actual Test Temperature', color='blue', linewidth=2)\nax.plot(test_dates, test_predict, label='Predicted Test Temperature', color='red', linestyle='--', linewidth=2)\n\nax.set_title('Temperature Forecast vs Actuals', fontsize=20)\nax.set_xlabel('Date', fontsize=14)\nax.set_ylabel('Temperature (Â°C)', fontsize=14)\nax.legend(fontsize=12)\nax.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}